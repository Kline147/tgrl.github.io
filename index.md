<link rel="stylesheet" href="styles.css">
<h1 align = "center">
TGRL:A Target-oriented Graph Reinforcement Learning method for Autonomous Whole-body Motion Planning in Dynamic Environments 
</h1>

<h2 align = "center">
Abstract
</h2>

Data-driven approaches have shown promising performance in safe and efficient trajectory planning for robotic arms, but mostly are limited to tasks in static environments. Challenges remain in complex dynamic environments such as dual-arm collaborative planning. Motion planners need to consider both end-effector trajectory planning and whole-body dynamic obstacle avoidance simultaneously. To this end, we propose a method TGRL. First, a target-oriented directed random graph is constructed in the configuration space. Then, elements of the environment and prior information are encoded and incorporated into the graph structure and node and edge embeddings are updated by means of message passing to obtain a motion planning sampled graph that incorporates global information. On this basis, we design a reinforcement learning framework for training to achieve optimization and enhance performance by intelligently selecting the next node based on the current node state to obtain a complete optimized planning path. Finally, our method was verified in both virtual and real environments. Our method significantly reduces the number of collision detections by 66.7%-75% compared to GNN\_TE and RRT and by nearly 99.9% compared to SIPP, resulting in  faster planning and consistently high success rates.

<h2 align = "left">
Approach overview
</h2>

![architecture](imgs/architecture.jpg)

The overall framework of the proposed TGRL: (1)Graph Generator firstly constructs TDRGG, then encodes the node and edge information of TDRGG along with obstacle information. By incorporating an attention mechanism for message passing, it generates a preliminary path planning sampling graph. (2)Motion Planner first stores path-related states and actions generated by the SIPP algorithm into an experience replay pool. It then takes the neighbor edge embeddings of the current node and robot information as input to generate the index of the next node, iteratively refines the sampling graph, and controls the robot through joint controller. (3)Specially, the formation of TDRGG should be noticed. First, n points are randomly sampled in the configuration space and the start and target points are added. Next, the directed RGG is generated by the k-NN algorithm. Finally the TDRGG is formed by connecting all the points and target point.

<div style="background-color: #c6d2ff; height: 60px; line-height: 60px; text-align: center; color: white; font-size: 24px;"> Simulation Experiments </div>

<h3 class="arm-title">Simulated arms (Degree of Freedom: 2)</h3>
<div class="image-row">
  <img src="gifs/simple_arm/test_9.gif" alt="Arm 1 DOF 1 GIF 1" class="side-by-side-img">
  <img src="gifs/simple_arm/test_11.gif" alt="Arm 1 DOF 1 GIF 1" class="side-by-side-img">
</div>

<h3 class="arm-title">KUKA iiwa (Degree of Freedom: 2)</h3>
<div class="image-row">
  <img src="gifs/KUKA/2/real_kuka_2dof.gif" alt="Arm 1 Type B DOF 2 GIF 1" class="side-by-side-img">
  <img src="gifs/KUKA/2/real_kuka_2dof2.gif" alt="Arm 1 Type B DOF 2 GIF 2" class="side-by-side-img">
</div>



<div style="background-color: #c6d2ff; height: 60px; line-height: 60px; text-align: center; color: white; font-size: 24px;"> Real World Experiments </div>

